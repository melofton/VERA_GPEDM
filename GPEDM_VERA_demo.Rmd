---
title: "GPEDM demo for VERA forecast challenge"
author: "Lofton, M.E., Thomas, R.Q., Munch, S.B."
date: "2025-02-19"
output: html_document
---

### Install and load GPEDM and other packages
```{r setup, include=FALSE}
# install packages on CRAN
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("devtools")) install.packages("devtools")
if (!require("knitr")) install.packages("knitr")

# install GPEDM and vera4castHelpers packages from GitHub
# you will need to install devtools first if you do not have it
devtools::install_github("tanyalrogers/GPEDM")
devtools::install_github("LTREB-reservoirs/vera4castHelpers")

# load packages
library(GPEDM)
library(tidyverse)
library(vera4castHelpers)

# set document options
knitr::opts_chunk$set(echo = TRUE)
```

### Read in and visualize VERA forecast challenge data
```{r}
# read in VERA forecast challenge data
url <- "https://renc.osn.xsede.org/bio230121-bucket01/vera4cast/targets/project_id=vera4cast/duration=P1D/daily-insitu-targets.csv.gz"
targets <- read_csv(url, show_col_types = FALSE)

# isolate water temperature data
# this is water temperature from the EXO sonde at FCR at 1.6 m depth
temp <- targets |>
  filter(variable == "Temp_C_mean" & site_id == "fcre" & depth_m == 1.6) %>%
  filter(!year(datetime) == 2018) # filter out 2018 because no data until August

# plot data
ggplot(data = temp, aes(x = datetime, y = observation)) +
  geom_line() +
  theme_bw()
```

### Visualize autocorrelation lags in the target variable
The purpose of this is to inform the choice of tau. **Tau** is the *length of the timestep* used in the model. Considerations in choosing tau include:    

1. **Forecasting needs** - over what time horizons are we trying to forecast? For example, if we have 10-minute data, probably a tau of 1 is not very useful because it would just be using data 10 minutes ago to predict data 10 minutes into the future. However, if we have daily data and we want a forecast for tomorrow, using a tau of 60 days might not be very useful because the model would only be using data that occurred two months ago to make a forecast of tomorrow (looking at more recent data would be beneficial).

*Note: In the default implementation, tau and the forecast horizon are assumed the same, but there is no reason this has to be the case, nor is there is any reason that tau has to be uniform, as you know. If you specify `E` and `tau` in `fitGP`, you get the following model:*

$$y_{t}=f(y_{t-\tau},y_{t-2\tau},...,y_{t-E\tau})$$

*However, you can produce any generic model by specifying the lags accordingly under `x`, and adjusting the inputs by some forecast horizon if you want to decouple it from tau.*

$$y_{t+h}=f(y_{t},y_{t-\tau_1},y_{t-\tau_2}...)$$

*or equivalently*

$$y_{t}=f(y_{t-h},y_{t-\tau_1-h},y_{t-\tau_2-h}...)$$

2. **Autocorrelation in the data** - if we want to avoid just relying on autocorrelation for our predictive capacity, we might consider choosing a larger value of tau; one way to assess this is to visualize the ACF of the target variable and choose a tau that is approximately the minimum (or where ACF approaches 0 or levels off). Tau shouldn't be so incredibly sensitive that changing it from, e.g., 88 to 90 to 92 days should make a huge difference in model fit.    

```{r}
acf(temp$observation, na.action = na.pass, lag.max = 100)
```

Here, we observe that ACF reaches a minimum at ~90 days, so that's potentially a good lag to use. However, we also want to make day-ahead forecasts (so we definitely want to use a 1-day lag to make this easy given the way the GPEDM package functions work), and there might be useful information at shorter lags than 90 days. So, instead of just setting tau to 90, we will use expert opinion to manually select lags that we think will be good for prediction. Here, we use expert opinion to choose lags of 1, 30, and 90 days. Ninety days also corresponds to about the residence time of FCR, which is another reason in support of choosing this lag.

Alternatively, we could use the ACF to select tau (90 days) and then select E, an embedding number that would tell the model how many 90-day lags to include. For example, a tau of 90 and an E of 2 would mean that the model would include a 90-day and 180-day lag. Typically, it is good practice to have `E*tau` = the dominant periodicity in the variable. For water temperature, this would be 1 year. So if we choose a tau of 90, we should choose an E of 4, giving `E*tau` of 360, or about a year. Conversely, if we chose a tau of 60, we would choose an E of 6.

### Data wrangling to fit EDM and forecast
First, we will extend the dataframe to include the next 1-35 days to supply that as `newdata` to the `fitGP` function below.

*The `makelags` function has a `forecast` argument, but it will only make a forecast table for tau steps into the future, so your approach here is acceptable. Also, playing around with this, this forecast feature will only work with your data if you put it in `as.data.frame`. Your data object here is not a standard data frame, so it's not playing well with the code because `df[,"Time"]` produces another `tbl_df`, not a vector, so it doesn't index properly. I can probably patch that though.*

```{r, eval=FALSE}
test1 <- temp |> add_column(Time = c(1:nrow(temp)))
class(test1)
#ok
head(makelags(as.data.frame(test1),"observation",E=3, tau=1, time="datetime",append = T))
#weird
makelags(as.data.frame(test1),"observation",E=3, tau=1, forecast = T, time="datetime")
#ok
makelags(as.data.frame(test1),"observation",E=3, tau=1, forecast = T, time="Time")
```


```{r}
forecast_time_horizon <- 35
forecast_dates <- seq.Date(from = as.Date(last(temp$datetime))+1, to = as.Date(last(temp$datetime))+forecast_time_horizon, by = "day")
fc_df <- data.frame(project_id = "vera4cast",
                    site_id = "fcre",
                    datetime = forecast_dates,
                    duration = "P1D",
                    depth_m = 1.6,
                    variable = "Temp_C_mean",
                    observation = NA)
mod_df <- bind_rows(temp, fc_df)
```

Because we are manually choosing lags that are not evenly spaced, we will calculate all lags from 1-360 days and then tell the model which ones to use when fitting. We also add a Time column which we *think* is needed for the function to run properly, as we were getting errors without it.

*If you omit the time column it should work fine, and it does for me below. If you can reproduce the error you were getting, that could be helpful.*

```{r}
#generate all lags
HPlags <- makelags(mod_df,"observation",E=360, tau=1, append=T) |>
  add_column(Time = c(1:nrow(mod_df)))
```

Here, we split the data into training and forecasting sets. We will need to select a forecast time horizon to do this. We are starting with a maximum forecast time horizon of 35 days, so we are generating predictions from 1-35 days into the future.
```{r}
# set forecast time horizon
forecast_time_horizon = 35

# split training and forecast sets
past_data  <- HPlags[1:(nrow(HPlags) - forecast_time_horizon),]
future_data  <- HPlags[(nrow(HPlags) - forecast_time_horizon + 1):nrow(HPlags),]
```

### Fit EDM model

Here we fit the EDM model with the following arguments:
1. *data* is the dataframe used for model training
2. *y* is the column name of the target variable as a character string
3. *x* are the column names of the lags used in prediction as a vector of character strings
4. *time* is the name of the timestep column (can this handle datetimes?)
5. *newdata* is the dataframe of timesteps to be forecasted

*Using a datetime for time will probably work, except in some cases where timesteps are iterated internally, like using the `forecast=T` feature in `makelags`, which wasn't designed to work with datetimes. I haven't fully tested it though, so a numeric index is probably safer.*

```{r}
mod <- fitGP(data = past_data, 
          y = "observation",
          x = c("observation_1","observation_30","observation_90"),
          time = "Time")
```

### Make an empty forecast dataframe
```{r}
fc_final <- data.frame(project_id = "vera4cast",
                       model_id = "example_GPEDM",
                       datetime = rep(forecast_dates, each = 2),
                       reference_datetime = Sys.Date(),
                       duration = "P1D",
                       site_id = "fcre",
                       depth_m = 1.6,
                       family = "normal",
                       parameter = rep(c("mu","sigma"), times = length(forecast_dates)),
                       variable = "Temp_C_mean",
                       prediction = NA) |> 
  slice(0)
```


### Run for-loop to get iterative predictions

`fitGP` can't handle the forecast dataframe without the lags pre-populated, so we will need to generate our predictions iteratively and fill the lags with model predictions as we go.

*The function `predict_iter` does iterated predictions, but the lags have to be evenly spaced for this wrapper to work. What you have below is an acceptable solution.*

```{r}
for(t in 1:nrow(future_data)){
  pred_list <- predict(mod, newdata = future_data[t,])
  pred_df <- pred_list$outsampresults
  
  if(t != nrow(future_data)){
  future_data[t+1,"observation_1"] <- pred_df$predmean
  }
  
  if(is.na(future_data[t+1,"observation_30"]) & t != nrow(future_data)){
    lag_30 <- fc_final |>
      filter(datetime == forecast_dates[t-29] & parameter == "mu") %>%
      pull(prediction)
    future_data[t+1,"observation_30"] <- lag_30
  }
  
  fc_temp <- data.frame(project_id = "vera4cast",
                       model_id = "example_GPEDM",
                       datetime = rep(forecast_dates[t],2),
                       reference_datetime = Sys.Date(),
                       duration = "P1D",
                       site_id = "fcre",
                       depth_m = 1.6,
                       family = "normal",
                       parameter = c("mu","sigma"), 
                       variable = "Temp_C_mean",
                       prediction = c(pred_df$predmean, pred_df$predsd)
                       )
  
  fc_final <- bind_rows(fc_final, fc_temp)
}
```

### Fill any NAs in the forecast
```{r}
fc_final <- fc_final %>%
  group_by(parameter) %>%
  tidyr::fill(prediction, .direction = "downup") %>%
  ungroup()
```

### Plot forecast
Just for funsies!
```{r}
fc_plot_data <- fc_final |>
  pivot_wider(names_from = "parameter", values_from = "prediction") |>
  mutate(lower = mu - 1.96*sigma,
         upper = mu + 1.96*sigma)

ggplot(data = fc_plot_data)+
  geom_ribbon(aes(x = datetime, ymin = lower, ymax = upper,
                  fill = "water temperature"), alpha = 0.5) +
  geom_line(data = past_data, aes(x = as.Date(datetime), y = observation), color="black") +
  geom_line(aes(x = datetime, y = mu, col = "water temperature")) +
  theme_bw() +
  scale_color_manual(name = "variable", values = c("water temperature" = "blue")) +
  scale_fill_manual(name = "variable", values = c("water temperature" = "blue")) +
  ylab("°C") +
  xlab("")
```

*It's worth looking at the summary and conditional responses. The model has mostly reverted to a persistence model,* $y_t=y_{t-1} + \epsilon$

```{r}
summary(mod)

getconditionals(mod)
```

### Submit forecast to VERA!
```{r}
theme <- 'daily'
date <- Sys.Date()
forecast_file <- paste(theme, date, "GPEDM", sep = '-')
forecast_file_name <- c(paste0(forecast_file, ".csv"))
write.csv(fc_final, forecast_file_name, row.names = FALSE)
vera4castHelpers::forecast_output_validator(forecast_file_name)
vera4castHelpers::submit(forecast_file_name, s3_region = "submit", s3_endpoint = "ltreb-reservoirs.org", first_submission = FALSE)

```


